{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c60aa41-76f6-4859-ae01-c53a1aee8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#presuming localCPU test runs and all the bits there are installed.\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "#from ctransformers import AutoModelForCausalLM\n",
    "# Don't use ctransformers since their cuda is 12.2 as per https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/discussions/23\n",
    "# !pipx install ctransformers[cuda]\n",
    "from tqdm.notebook import tqdm\n",
    "from distributed import Client, LocalCluster\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "# https://docs.mlerp.cloud.edu.au/tutorials/3_dask_pytorch.html\n",
    "\n",
    "\n",
    "DATASET = [\"What are the planets of the solar system?\", \n",
    "           \"Why has the Moon such an influence upon Earth?\",\n",
    "           \"Can you write a poem about the Moon?\",\n",
    "           \"How could we travel to the Moon? With a big cannon?\",\n",
    "           \"Who is the most significant thinker in our canon?\",\n",
    "           \"What is the role of the artes liberales?\"]\n",
    "\n",
    "batch_size = len(DATASET)\n",
    "\n",
    "# https://jobqueue.dask.org/en/latest/debug.html\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48af7b5c-dbc5-4ccc-873d-93a580d1360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to make sure we download the model *before* we share this out.\n",
    "# MODEL_FILE=\"monadgpt.Q4_K_M.gguf\"\n",
    "# MODEL_ID=\"TheBloke/MonadGPT-GGUF\"\n",
    "MODEL_ID=\"Pclanglais/MonadGPT\"\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import pipeline\n",
    "\n",
    "#!huggingface-cli download \"TheBloke/MonadGPT-GGUF\" \"monadgpt.Q4_K_M.gguf\"\n",
    "#!huggingface-cli download \"Pclanglais/MonadGPT\"\n",
    "\n",
    "# make sure to run this ahead of time, we need to get the local files cached before sending out.\n",
    "\n",
    "def generate(loader, output=[]): \n",
    "    #tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "    # llm = AutoModelForCausalLM.from_pretrained(MODEL_ID, \n",
    "    #                                            #model_file=MODEL_FILE, \n",
    "    #                                           #model_type=\"mistral\", \n",
    "    #                                           #gpu_layers=100,#)\n",
    "    #                                           torch_dtype=torch.bfloat16, \n",
    "    #                                           device_map=\"auto\",\n",
    "    #                                           )\n",
    "                                              #attn_implementation=\"flash_attention_2\",)\n",
    "                                              # System nvcc must be at least 11.6 https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features\n",
    "    # pipe = pipeline('text-generation', model='Pclanglais/MonadGPT', device='cuda')\n",
    "\n",
    "\n",
    "    for line in loader:\n",
    "        prompt = f\"\"\"<|im_start|>system\\nYou are MonadGPT, a very old chatbot from the 17th century. Please answer the questions using an archaic language<|im_end|>\n",
    "<|im_start|>user\\n{line}<|im_end|>\\n<|im_start|>assistant\\n\"\"\"\n",
    "        print(\"working on\", prompt)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_ID, \n",
    "                                                    device_map=\"auto\",\n",
    "                                                    torch_dtype=torch.bfloat16,\n",
    "                                                    load_in_8bit=True,\n",
    "                                                    # model_type=\"mistral\",                \n",
    "                                                    )\n",
    "        # model.to(\"cuda\")\n",
    "        #Avoids https://stackoverflow.com/questions/66091226/runtimeerror-expected-all-tensors-to-be-on-the-same-device-but-found-at-least\n",
    "        \n",
    "    \n",
    "        # text = \"Hello my name is\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        outputs = model.generate(**inputs)\n",
    "        output.append({\"prompt\":prompt,\"output\":outputs})\n",
    "        # output.append({\"prompt\":prompt,\"output\":pipe(prompt)})\n",
    "        print(output)\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4556f92-436c-420a-a08c-0364ae8ee633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:Using selector: EpollSelector\n",
      "DEBUG:Starting worker: SLURMCluster-0\n",
      "DEBUG:writing job script: \n",
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -e ./logs/dask-worker-%J.err\n",
      "#SBATCH -o ./logs/dask-worker-%J.out\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=8\n",
      "#SBATCH --mem=120G\n",
      "#SBATCH -t 00:30:00\n",
      "#SBATCH --gres=gpu:40gb:1\n",
      "\n",
      "/apps/mambaforge/envs/detectron-sam/bin/python -m distributed.cli.dask_worker tcp://192.168.0.211:34841 --nthreads 8 --memory-limit 119.21GiB --name SLURMCluster-0 --no-nanny --death-timeout 60\n",
      "\n",
      "DEBUG:Executing the following command to command line\n",
      "sbatch /tmp/tmpgj55ybu_.sh\n",
      "DEBUG:Starting job: 2597\n"
     ]
    }
   ],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client, progress\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "try:\n",
    "    client.shutdown()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "shutil.rmtree(\"logs/\", ignore_errors=True)\n",
    "os.mkdir(\"logs\")\n",
    "\n",
    "try:\n",
    "    # cluster = LocalCluster(processes=False)\n",
    "    cluster = SLURMCluster(\n",
    "        memory=\"192g\", processes=1, cores=8, job_extra_directives=[\"--gres=gpu:40gb:1\",], nanny=False,\n",
    "        # queue=\"lion\", # check with $ sacctmgr list clusters\n",
    "        log_directory=\"./logs\", #no trailing slash\n",
    "        \n",
    "    )\n",
    "    cluster.scale(1)\n",
    "    client = Client(cluster)\n",
    "    # dataloader_future = client.scatter(dataloader)\n",
    "    # future = client.submit(generate, dataloader_future)\n",
    "    future = client.submit(generate, DATASET)\n",
    "    # Don't use client.map here. It causes OOM errors because distributed tries to run *multiple* instances of the model at the same time. :(\n",
    "except:\n",
    "    client.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0d52460-22e7-44d2-9b2d-14c16ef092fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTITIO            NAME    STATE       TIME    TIME_LEFT CPUS MIN_MEM NODELIST(REASON) QOS\n",
      " BigCats     Jupyter Lab  RUNNING    1:04:37        55:23    6     32G mlerp-monash-node03 lion\n",
      " BigCats     dask-worker  RUNNING       0:04        29:56    8    120G mlerp-monash-node03 cheetah\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65f1d131cea4f38be4d76925b401e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!squeue --me --format \"%.8P %.15j %.8T %.10M %.12L %.4C %.7m %R %q\"\n",
    "progress(future)\n",
    "# If running the large model, progress won't work as well as tail -f the log file in ./logs\n",
    "# Looks like monad is at least a 5GB download, so make sure to watch with tail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b900b04-b87b-49ec-876c-e923c9bc50cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "result = future.result()\n",
    "for row in result:\n",
    "    print(row)\n",
    "pprint(result)\n",
    "# https://distributed.dask.org/en/stable/quickstart.html#gather need to use gather instead\n",
    "#outputs = client.gather(iter(futuremap))\n",
    "#print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3512bb8d-9b75-4163-a59b-9ca412df228d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:Stopping worker: SLURMCluster-0 job: 2597\n",
      "DEBUG:Executing the following command to command line\n",
      "scancel 2597\n",
      "DEBUG:Closed job 2597\n",
      "DEBUG:Executing the following command to command line\n",
      "scancel 2597\n",
      "DEBUG:Closed job 2597\n"
     ]
    }
   ],
   "source": [
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
